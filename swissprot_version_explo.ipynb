{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0910a9ba",
   "metadata": {},
   "source": [
    "This notebook explores how annotation transfer behaves in terms of leftover unannotated proteins lacking alignments according to the database (SwissProt).\n",
    "The idea is to understand how baseline annotation transfer methods (e.g. BLAST) performance evolves over time. This will help understand whether alignment-based methods are still considered a bad option for annotation transfer or if the increase in the number of sequences in databases has made relevant once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b661d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uniprot_sprot-only2010_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2010_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2010_01...\n",
      "Found 2010_01/uniprot_sprot.dat.gz, extracting to 2010_01/uniprot_sprot.dat...\n",
      "Extracted 2010_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2011_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2011_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2011_01...\n",
      "Found 2011_01/uniprot_sprot.dat.gz, extracting to 2011_01/uniprot_sprot.dat...\n",
      "Extracted 2011_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2012_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2012_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2012_01...\n",
      "Found 2012_01/uniprot_sprot.dat.gz, extracting to 2012_01/uniprot_sprot.dat...\n",
      "Extracted 2012_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2013_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2013_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2013_01...\n",
      "Found 2013_01/uniprot_sprot.dat.gz, extracting to 2013_01/uniprot_sprot.dat...\n",
      "Extracted 2013_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2014_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2014_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2014_01...\n",
      "Found 2014_01/uniprot_sprot.dat.gz, extracting to 2014_01/uniprot_sprot.dat...\n",
      "Extracted 2014_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2015_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2015_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2015_01...\n",
      "Found 2015_01/uniprot_sprot.dat.gz, extracting to 2015_01/uniprot_sprot.dat...\n",
      "Extracted 2015_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2016_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2016_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2016_01...\n",
      "Found 2016_01/uniprot_sprot.dat.gz, extracting to 2016_01/uniprot_sprot.dat...\n",
      "Extracted 2016_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2017_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2017_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2017_01...\n",
      "Found 2017_01/uniprot_sprot.dat.gz, extracting to 2017_01/uniprot_sprot.dat...\n",
      "Extracted 2017_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2018_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2018_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2018_01...\n",
      "Found 2018_01/uniprot_sprot.dat.gz, extracting to 2018_01/uniprot_sprot.dat...\n",
      "Extracted 2018_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2019_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2019_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2019_01...\n",
      "Found 2019_01/uniprot_sprot.dat.gz, extracting to 2019_01/uniprot_sprot.dat...\n",
      "Extracted 2019_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2020_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2020_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2020_01...\n",
      "Found 2020_01/uniprot_sprot.dat.gz, extracting to 2020_01/uniprot_sprot.dat...\n",
      "Extracted 2020_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2021_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2021_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2021_01...\n",
      "Found 2021_01/uniprot_sprot.dat.gz, extracting to 2021_01/uniprot_sprot.dat...\n",
      "Extracted 2021_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2022_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2022_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2022_01...\n",
      "Found 2022_01/uniprot_sprot.dat.gz, extracting to 2022_01/uniprot_sprot.dat...\n",
      "Extracted 2022_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2023_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2023_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2023_01...\n",
      "Found 2023_01/uniprot_sprot.dat.gz, extracting to 2023_01/uniprot_sprot.dat...\n",
      "Extracted 2023_01/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only2024_01.tar.gz...\n",
      "Extracting uniprot_sprot-only2024_01.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2024_01...\n",
      "Found 2024_01/uniprot_sprot.dat.gz, extracting to 2024_01/uniprot_sprot.dat...\n",
      "Extracted 2024_01/uniprot_sprot.dat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2024\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    rel = f\"release-{year}_01\"\n",
    "    file = f\"uniprot_sprot-only{year}_01.tar.gz\"\n",
    "    url = f\"https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/{rel}/knowledgebase/{file}\"\n",
    "    outdir = f\"{year}_01\"\n",
    "\n",
    "    # Download if not already present\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"Downloading {file}...\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(file, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Extract tar.gz\n",
    "    print(f\"Extracting {file}...\")\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "        with tarfile.open(file, \"r:gz\") as tar:\n",
    "            tar.extractall(path=outdir)\n",
    "\n",
    "    # Find and gunzip uniprot_sprot.dat.gz\n",
    "    print(f\"Finding uniprot_sprot.dat.gz in {outdir}...\")\n",
    "    datgz_path = None\n",
    "    for root, dirs, files in os.walk(outdir):\n",
    "        for fname in files:\n",
    "            if fname == \"uniprot_sprot.dat.gz\":\n",
    "                datgz_path = os.path.join(root, fname)\n",
    "                break\n",
    "        if datgz_path:\n",
    "            break\n",
    "\n",
    "    if not datgz_path:\n",
    "        print(f\"No uniprot_sprot.dat.gz found for {year}\")\n",
    "        continue\n",
    "\n",
    "    dat_path = datgz_path[:-3]  # Remove .gz\n",
    "    print(f\"Found {datgz_path}, extracting to {dat_path}...\")\n",
    "    if not os.path.isfile(dat_path):\n",
    "        with gzip.open(datgz_path, 'rb') as f_in, open(dat_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Extracted {dat_path}\")\n",
    "\n",
    "    # Delete all .gz files in outdir\n",
    "    for root, dirs, files in os.walk(outdir):\n",
    "        for fname in files:\n",
    "            if fname.endswith('.gz'):\n",
    "                os.remove(os.path.join(root, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7263ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uniprot_sprot-only15.0.tar.gz...\n",
      "Extracting uniprot_sprot-only15.0.tar.gz...\n",
      "Finding uniprot_sprot.dat.gz in 2009_03...\n",
      "Found 2009_03/uniprot_sprot.dat.gz, extracting to 2009_03/uniprot_sprot.dat...\n",
      "Extracted 2009_03/uniprot_sprot.dat\n",
      "Downloading uniprot_sprot-only13.0.tar.gz...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m         r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 16\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n\u001b[1;32m     17\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "versions = [15.0, 13.0, 10.0, 7.0, 4.0, 1.0]\n",
    "date = ['2009_03', '2008_01', '2007_03', '2006_02', '2005_01', '2003_12']\n",
    "for version, date in zip(versions, date):\n",
    "    rel = f\"release{version}\"\n",
    "    file = f\"uniprot_sprot-only{version}.tar.gz\"\n",
    "    url = f\"https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/{rel}/knowledgebase/{file}\"\n",
    "    outdir = date\n",
    "\n",
    "    # Download if not already present\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"Downloading {file}...\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(file, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Extract tar.gz\n",
    "    print(f\"Extracting {file}...\")\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "        with tarfile.open(file, \"r:gz\") as tar:\n",
    "            tar.extractall(path=outdir)\n",
    "\n",
    "    # Find and gunzip uniprot_sprot.dat.gz\n",
    "    print(f\"Finding uniprot_sprot.dat.gz in {outdir}...\")\n",
    "    datgz_path = None\n",
    "    for root, dirs, files in os.walk(outdir):\n",
    "        for fname in files:\n",
    "            if fname == \"uniprot_sprot.dat.gz\":\n",
    "                datgz_path = os.path.join(root, fname)\n",
    "                break\n",
    "        if datgz_path:\n",
    "            break\n",
    "\n",
    "    if not datgz_path:\n",
    "        print(f\"No uniprot_sprot.dat.gz found for {year}\")\n",
    "        continue\n",
    "\n",
    "    dat_path = datgz_path[:-3]  # Remove .gz\n",
    "    print(f\"Found {datgz_path}, extracting to {dat_path}...\")\n",
    "    if not os.path.isfile(dat_path):\n",
    "        with gzip.open(datgz_path, 'rb') as f_in, open(dat_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Extracted {dat_path}\")\n",
    "\n",
    "    # Delete all .gz files in outdir\n",
    "    for root, dirs, files in os.walk(outdir):\n",
    "        for fname in files:\n",
    "            if fname.endswith('.gz'):\n",
    "                os.remove(os.path.join(root, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d85059",
   "metadata": {},
   "source": [
    "parse db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /home/atoffano/PFP_baselines/2024_01/uniprot_sprot.dat entries...\n",
      "Parsed 570830 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing entries content:  82%|████████▏ | 467581/570830 [00:44<00:09, 10543.38entry/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdat_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[43mparse_uniprot_dat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m     58\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntryID\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mEntry Name\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mparse_uniprot_dat\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAC \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     30\u001b[0m     entryid \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDR   GO;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#example line: DR   GO; GO:0046782; P:regulation of viral transcription; IEA:InterPro.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     m \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+GO;\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(GO:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+);\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "BASE_PATH = \"/home/atoffano/PFP_baselines\"\n",
    "\n",
    "def parse_uniprot_dat(filepath):\n",
    "    results = []\n",
    "    print(f\"Parsing {filepath} entries...\")\n",
    "    with open(filepath, \"r\") as f:\n",
    "        entry = []\n",
    "        for line in f:\n",
    "            if line.strip() == \"//\":\n",
    "                results.append(entry)\n",
    "                entry = []\n",
    "            else:\n",
    "                entry.append(line.rstrip())\n",
    "    parsed = []\n",
    "    print(f\"Parsed {len(results)} entries.\")\n",
    "    for entry in tqdm.tqdm(results, desc=\"Parsing entries content\", unit=\"entry\"):\n",
    "        uniprot_id = None\n",
    "        go_terms = []\n",
    "        sequence = \"\"\n",
    "        in_seq = False\n",
    "        for line in entry:\n",
    "            if line.startswith(\"ID \"):\n",
    "                #example line: ID 001R_FRG3G Reviewed; 256 AA.\n",
    "                uniprot_id = line.split()[1]\n",
    "            elif line.startswith(\"AC \"):\n",
    "                entryid = line.split()[1].strip(\";\")\n",
    "            elif line.startswith(\"DR   GO;\"):\n",
    "                #example line: DR   GO; GO:0046782; P:regulation of viral transcription; IEA:InterPro.\n",
    "                m = re.match(r\"DR\\s+GO;\\s*(GO:\\d+);\", line)\n",
    "                if m:\n",
    "                    go_terms.append(m.group(1))\n",
    "            elif line.startswith(\"SQ \"):\n",
    "                in_seq = True\n",
    "                continue\n",
    "            elif in_seq:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                sequence += \"\".join(line.strip().split())\n",
    "        if uniprot_id:\n",
    "            parsed.append((uniprot_id, entryid, \"; \".join(go_terms), sequence))\n",
    "    return parsed\n",
    "\n",
    "\n",
    "dates = ['2024_01', '2023_01', '2022_01', '2021_01', '2020_01', '2019_01', '2018_01', '2017_01', '2016_01', '2015_01', '2014_01', '2013_01', '2012_01', '2011_01', '2010_01', '2009_03', '2008_01', '2007_03', '2006_02', '2005_01', '2003_12']\n",
    "for date in dates:\n",
    "    year_folder = os.path.join(BASE_PATH, date)\n",
    "    dat_file = os.path.join(year_folder, \"uniprot_sprot.dat\")\n",
    "    output_file = os.path.join(year_folder, f\"swissprot_{date}_annotations.tsv\")\n",
    "    if not os.path.isfile(dat_file):\n",
    "        print(f\"File not found: {dat_file}\")\n",
    "        continue\n",
    "    entries = parse_uniprot_dat(dat_file)\n",
    "    with open(output_file, \"w\") as out:\n",
    "        out.write(\"EntryID\\tEntry Name\\tterm\\tSequence\\n\")\n",
    "        for uniprot_id, entryid, go_terms, sequence in entries:\n",
    "            out.write(f\"{uniprot_id}\\t{entryid}\\t{go_terms}\\t{sequence}\\n\")\n",
    "    print(f\"Wrote {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 2024_01...\n",
      "Found 0 entries in 2024_01 not present in 2024_01\n",
      "Checking 2023_01...\n",
      "Found 28 entries in 2023_01 not present in 2024_01\n",
      "Checking 2022_01...\n",
      "Found 75 entries in 2022_01 not present in 2024_01\n",
      "Checking 2021_01...\n",
      "Found 173 entries in 2021_01 not present in 2024_01\n",
      "Checking 2020_01...\n",
      "Found 237 entries in 2020_01 not present in 2024_01\n",
      "Checking 2019_01...\n",
      "Found 3028 entries in 2019_01 not present in 2024_01\n",
      "Checking 2018_01...\n",
      "Found 3065 entries in 2018_01 not present in 2024_01\n",
      "Checking 2017_01...\n",
      "Found 3114 entries in 2017_01 not present in 2024_01\n",
      "Checking 2016_01...\n",
      "Found 3216 entries in 2016_01 not present in 2024_01\n",
      "Checking 2015_01...\n",
      "Found 3290 entries in 2015_01 not present in 2024_01\n",
      "Checking 2014_01...\n",
      "Found 5121 entries in 2014_01 not present in 2024_01\n",
      "Checking 2013_01...\n",
      "Found 5020 entries in 2013_01 not present in 2024_01\n",
      "Checking 2012_01...\n",
      "Found 5026 entries in 2012_01 not present in 2024_01\n",
      "Checking 2011_01...\n",
      "Found 6027 entries in 2011_01 not present in 2024_01\n",
      "Checking 2010_01...\n",
      "Found 6236 entries in 2010_01 not present in 2024_01\n",
      "Checking 2009_03...\n",
      "Found 6290 entries in 2009_03 not present in 2024_01\n",
      "Checking 2008_01...\n",
      "Found 5731 entries in 2008_01 not present in 2024_01\n",
      "Checking 2007_03...\n",
      "Found 5707 entries in 2007_03 not present in 2024_01\n",
      "Checking 2006_02...\n",
      "Found 5392 entries in 2006_02 not present in 2024_01\n",
      "Checking 2005_01...\n",
      "Found 7480 entries in 2005_01 not present in 2024_01\n",
      "Checking 2003_12...\n",
      "Found 9609 entries in 2003_12 not present in 2024_01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_PATH = \"/home/atoffano/PFP_baselines\"\n",
    "\n",
    "dates = ['2024_01', '2023_01', '2022_01', '2021_01', '2020_01', '2019_01', '2018_01', '2017_01', '2016_01', '2015_01', '2014_01', '2013_01', '2012_01', '2011_01', '2010_01', '2009_03', '2008_01', '2007_03', '2006_02', '2005_01', '2003_12']\n",
    "\n",
    "# Ref file: last version\n",
    "ref_file = os.path.join(BASE_PATH, \"2024_01/swissprot_2024_01_annotations.tsv\")\n",
    "with open(ref_file) as f:\n",
    "    ref_ids = set(line.split('\\t')[1] for i, line in enumerate(f) if i > 0)\n",
    "\n",
    "for date in dates:\n",
    "    tsv_file = os.path.join(BASE_PATH, f\"{date}\", f\"swissprot_{date}_annotations.tsv\")\n",
    "    if not os.path.isfile(tsv_file):\n",
    "        continue\n",
    "    # Read all lines\n",
    "    with open(tsv_file) as f:\n",
    "        lines = f.readlines()\n",
    "    header = lines[0]\n",
    "    filtered_lines = [header]\n",
    "    for line in lines[1:]:\n",
    "        entry_id = line.split('\\t')[1]\n",
    "        if entry_id in ref_ids:\n",
    "            filtered_lines.append(line)\n",
    "    # Overwrite file with filtered entries\n",
    "    with open(tsv_file, \"w\") as f:\n",
    "        f.writelines(filtered_lines)\n",
    "    print(f\"Filtered {tsv_file}: {len(filtered_lines)-1} entries kept among {len(lines)-1} original entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caa99d",
   "metadata": {},
   "source": [
    "create fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85840a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "for date in dates:\n",
    "    tsv_file = os.path.join(BASE_PATH, f\"{date}\", f\"swissprot_{date}_annotations.tsv\")\n",
    "    fasta_file = os.path.join(BASE_PATH, f\"{date}\", f\"swissprot_{date}.fasta\")\n",
    "    if not os.path.isfile(tsv_file):\n",
    "        print(f\"TSV not found: {tsv_file}\")\n",
    "        continue\n",
    "    # Read TSV and write FASTA\n",
    "    records = []\n",
    "    with open(tsv_file) as f:\n",
    "        next(f)  # skip header\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split('\\t')\n",
    "            entry_id, entry_name, go_terms, sequence = parts\n",
    "            if not sequence:\n",
    "                continue\n",
    "            record = SeqRecord(Seq(sequence), id=entry_name, description=\"\")\n",
    "            records.append(record)\n",
    "    SeqIO.write(records, fasta_file, \"fasta\")\n",
    "    print(f\"Wrote {len(records)} records to {fasta_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a18c8",
   "metadata": {},
   "source": [
    "Term propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c03bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import obonet\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "\n",
    "os.chdir('/home/atoffano/these-antoine/')\n",
    "from utils.preprocessing import obsolete_terms, alt_id_terms\n",
    "from utils.ia import propagate_terms, clean_ontology_edges, fetch_aspect\n",
    "\n",
    "# Load GO ontology\n",
    "obo_file_path = \"/home/atoffano/these-antoine/data/cafa/Train/go-basic.obo\"\n",
    "ontology_graph = obonet.read_obo(obo_file_path)\n",
    "ontology_graph = clean_ontology_edges(ontology_graph)\n",
    "\n",
    "# Get obsolete and alternative terms\n",
    "obsolete, old_to_new = obsolete_terms(obo_file_path)\n",
    "alt_to_term = alt_id_terms(obo_file_path)\n",
    "\n",
    "# Get subontologies and mappings\n",
    "roots = {'BPO': 'GO:0008150', 'CCO': 'GO:0005575', 'MFO': 'GO:0003674'}\n",
    "subontologies = {aspect: fetch_aspect(ontology_graph, roots[aspect]) for aspect in roots}\n",
    "aspect2go = {aspect: list(subontologies[aspect].nodes) for aspect in roots}\n",
    "go2aspect = {go: aspect for aspect, go_list in aspect2go.items() for go in go_list}\n",
    "\n",
    "ontologies = {'BPO': 'bp', 'CCO': 'cc', 'MFO': 'mf'}\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    print(f\"Checking {date}...\")\n",
    "    tsv_file = os.path.join(BASE_PATH, f\"{date}\", f\"swissprot_{date}_annotations.tsv\")\n",
    "    # Load df\n",
    "    swissprot = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "    swissprot = swissprot[['Entry Name', 'term']]\n",
    "    # Rename Entry Name to EntryID\n",
    "    swissprot = swissprot.rename(columns={'Entry Name': 'EntryID'})\n",
    "    swissprot['term'] = swissprot['term'].str.replace(' ', '').str.split(';')\n",
    "\n",
    "    df_exploded = swissprot.explode('term').dropna()\n",
    "    df_exploded['aspect'] = df_exploded['term'].map(go2aspect)\n",
    "    df_exploded = df_exploded.dropna(subset=['aspect'])\n",
    "\n",
    "    df_exploded = df_exploded.drop_duplicates()\n",
    "    df_exploded = df_exploded[df_exploded['term'].notna()]\n",
    "\n",
    "    # Split according to aspect\n",
    "    for aspect in ['BPO', 'CCO', 'MFO']:\n",
    "        df_aspect = df_exploded[df_exploded['aspect'] == aspect].drop_duplicates()\n",
    "        # Propagate annotations\n",
    "        print('Propagating annotations...')\n",
    "        df_prop = propagate_terms(df_aspect, {aspect: subontologies[aspect]})\n",
    "\n",
    "        # Group terms by protein\n",
    "        df_grouped = df_prop.groupby('EntryID')['term'].apply(list).reset_index()\n",
    "        \n",
    "        # Save to TSV\n",
    "        output_filename = os.path.join(BASE_PATH, f\"{date}\", f\"swissprot_{date}_{ontologies[aspect]}_annotations.tsv\")\n",
    "        df_grouped.to_csv(output_filename, sep='\\t', index=False)\n",
    "        print(f\"Saved {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f90360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment of protein sequences: Version X versus Ref. Version (2024_01)\n",
    "# Run Diamond blast on protein sequences against themselves\n",
    "diamond_path = \"/home/atoffano/these-antoine/utils\"\n",
    "print(\"Creating Diamond database...\")\n",
    "\n",
    "for date in dates:\n",
    "    datapath = os.path.join(BASE_PATH, date)\n",
    "    fasta_file = os.path.join(datapath, f\"swissprot_{date}.fasta\")\n",
    "    if not os.path.isfile(fasta_file):\n",
    "        print(f\"FASTA file not found: {fasta_file}\")\n",
    "        continue\n",
    "    subprocess.run(\n",
    "        f\"{diamond_path}/diamond makedb --in {fasta_file} -d {datapath}/{date}_proteins_set\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    print(\"Running Diamond blast on protein sequences against themselves...\")\n",
    "    subprocess.run(\n",
    "        f\"{diamond_path}/diamond blastp --very-sensitive --db {datapath}/{date}_proteins_set.dmnd --query {fasta_file} --out {datapath}/diamond_swissprot_alignment_{date}.tsv -e 0.001\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
